{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Task:\n",
    "\n",
    "- well defined ending point\n",
    "- sparse reward (for example: chess): Reward only at the end of all actions/states\n",
    "\n",
    "Interaction ends at some time step T.\n",
    "\n",
    "S_0 A_0 R_1 S_1 A_1, ..., R_T, S_T Game Over -> Evaluate the max score reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Task\n",
    "- Agent lives forever\n",
    "\n",
    "Example: Stock Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hypothesis\n",
    "\n",
    "- All goals can be framed as the maximization of expected cumulative reward\n",
    "\n",
    "Example of the walking robot:\n",
    "\n",
    "**Actions:** Forces applied to joints\n",
    "\n",
    "**States:** The positions and velocities of the joints, statistics about ground, and foot sensor data\n",
    "\n",
    "![explanation reward for humanoid walking agent](images/rl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Return\n",
    "\n",
    "The Agent maximizes not the individual step's return, but the expected return G_t of all future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "- Reward sooner is valued higher -> Discounting future returns.\n",
    "- $\\gamma$ >= 0 and <= 1 => $\\gamma^2$ * $R_2$, $\\gamma^3$ * $R_3$, ...\n",
    "- Important for continuing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "![overview of MDPs](images/rl2.png)\n",
    "\n",
    "#### Excercise and Mathematical notation\n",
    "\n",
    "![mathematical expression of options](images/rl3.png)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A (finite) MDP is defined by:\n",
    "\n",
    "- a (finite) set of states\n",
    "- a (finite) set of actions\n",
    "- a (finite) set of rewards\n",
    "- the one-step dynamics of the environment (formula)\n",
    "- the discount rate $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy desribes the Mapping of States to Actions:\n",
    "\n",
    "- deterministic policy: $\\pi$:S -> A\n",
    "- stochastic policy: $\\pi$:S x A -> [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value function for a policy](images/rl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bellmann Equation Pt1](images/rl5.png)\n",
    "![Bellmann Equation Pt1](images/rl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-Value Function vs. Action-Value Function\n",
    "\n",
    "![State-Value Function vs. Action-Value Function comparison](images/rl7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Policy Evaluation\n",
    "\n",
    "1. Set the $v_\\pi$(s) of each state to 0\n",
    "2. Iterate through the states and Update the guesses of each state:\n",
    "V($s_1$) <- $\\frac{1}{2}$ x (-1 + V($s_2$)) + $\\frac{1}{2}$ x (-1 + V($s_3$)) | Bellmann Equation\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "![Pseudocode to update the value function with the bellmann equation](images/rl8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the action-value function:\n",
    "\n",
    "![example to calculate the action-value function](images/rl9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "The first step is the action-value function that we calculated\n",
    "\n",
    "![How to get to the policy improvement](images/rl10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated Policy Iteration\n",
    "\n",
    "- Stop the iteration after a max number of steps instead of theta/no changes in the policy improvement loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning vs. Dynamic Programming setting: Agent has no knowledge about the environment dynamics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo methods are ways of solving Reinforcement Learning problems based on averaging sample returns\n",
    "- Experience is divided into eposides and each eposisode ends\n",
    "- For each episode the agent's goal is to find the optimal policy to maximize the expected cumulative reward\n",
    "\n",
    "### The Prediction Problem\n",
    "\n",
    "**Off-Policy Method**\n",
    "- When the agent uses another policy to evaluate the environment as to maximize => b != $\\pi$\n",
    "\n",
    "**On-Policy Method**\n",
    "- The same policy is used to evaluate as well as to maximize -> $\\pi$\n",
    "\n",
    "**Value Function in MC**\n",
    "- Look at each occurance of a state\n",
    "- Sum of the rewards following the observed state\n",
    "- Divide by the number of episodes the state was observed in\n",
    "\n",
    "Now it depends on first-visit or every-visit MC. Should all occurences of a state be considered, or only the first one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Value from MCs\n",
    "\n",
    "- No reuse of algorithm possible since we do not have a one-step dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the optimal policy $\\pi$***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Incremental Mean computational efficient](images/rl11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy Policy\n",
    "\n",
    "![Epsilon Greedy Policy](images/rl12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
