{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Task:\n",
    "\n",
    "- well defined ending point\n",
    "- sparse reward (for example: chess): Reward only at the end of all actions/states\n",
    "\n",
    "Interaction ends at some time step T.\n",
    "\n",
    "S_0 A_0 R_1 S_1 A_1, ..., R_T, S_T Game Over -> Evaluate the max score reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Task\n",
    "- Agent lives forever\n",
    "- Example: Stock Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hypothesis\n",
    "\n",
    "- All goals can be framed as the maximization of expected cumulative reward\n",
    "\n",
    "Example of the walking robot:\n",
    "\n",
    "**Actions:** Forces applied to joints\n",
    "\n",
    "**States:** The positions and velocities of the joints, statistics about ground, and foot sensor data\n",
    "\n",
    "![explanation reward for humanoid walking agent](images/rl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Return\n",
    "\n",
    "The Agent maximizes not the individual step's return, but the expected return G_t of all future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "- Reward sooner is valued higher -> Discounting future returns.\n",
    "- $\\gamma$ >= 0 and <= 1 => $\\gamma^2$ * $R_2$, $\\gamma^3$ * $R_3$, ...\n",
    "- Important for continuing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "![overview of MDPs](images/rl2.png)\n",
    "\n",
    "#### Excercise and Mathematical notation\n",
    "\n",
    "![mathematical expression of options](images/rl3.png)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A (finite) MDP is defined by:\n",
    "\n",
    "- a (finite) set of states\n",
    "- a (finite) set of actions\n",
    "- a (finite) set of rewards\n",
    "- the one-step dynamics of the environment (formula)\n",
    "- the discount rate $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy desribes the Mapping of States to Actions:\n",
    "\n",
    "- **deterministic** policy: $\\pi$:S -> A\n",
    "- **stochastic** policy: $\\pi$:S x A -> [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value function for a policy](images/rl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bellmann Equation Pt1](images/rl5.png)\n",
    "![Bellmann Equation Pt1](images/rl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-Value Function vs. Action-Value Function\n",
    "\n",
    "![State-Value Function vs. Action-Value Function comparison](images/rl7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Policy Evaluation\n",
    "\n",
    "1. Set the $v_\\pi$(s) of each state to 0\n",
    "2. Iterate through the states and Update the guesses of each state:\n",
    "V($s_1$) <- $\\frac{1}{2}$ x (-1 + V($s_2$)) + $\\frac{1}{2}$ x (-1 + V($s_3$)) | Bellmann Equation\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "![Pseudocode to update the value function with the bellmann equation](images/rl8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the action-value function:\n",
    "\n",
    "![example to calculate the action-value function](images/rl9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "The first step is the action-value function that we calculated\n",
    "\n",
    "![How to get to the policy improvement](images/rl10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated Policy Iteration\n",
    "\n",
    "- Stop the iteration after a max number of steps instead of theta/no changes in the policy improvement loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning vs. Dynamic Programming setting: Agent has no knowledge about the environment dynamics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo methods are ways of solving Reinforcement Learning problems based on averaging sample returns\n",
    "- Experience is divided into eposides and each eposisode ends\n",
    "- For each episode the agent's goal is to find the optimal policy to maximize the expected cumulative reward\n",
    "\n",
    "### The Prediction Problem\n",
    "\n",
    "**Off-Policy Method**\n",
    "- When the agent uses another policy to evaluate the environment as to maximize => b != $\\pi$\n",
    "\n",
    "**On-Policy Method**\n",
    "- The same policy is used to evaluate as well as to maximize -> $\\pi$\n",
    "\n",
    "**Value Function in MC**\n",
    "- Look at each occurance of a state\n",
    "- Sum of the rewards following the observed state\n",
    "- Divide by the number of episodes the state was observed in\n",
    "\n",
    "Now it depends on first-visit or every-visit MC. Should all occurences of a state be considered, or only the first one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Value from MCs\n",
    "\n",
    "- No reuse of algorithm possible since we do not have a one-step dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the optimal policy $\\pi$***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Incremental Mean computational efficient](images/rl11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy Policy\n",
    "\n",
    "![Epsilon Greedy Policy](images/rl12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The larger the epsilon the more likely you choose the non-greedy option\n",
    "\n",
    "**In summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overview of epsilon-greey policies](images/rl13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration vs. Explotation dilemma\n",
    "\n",
    "- Should the Agent use the action that is known to maximize the reward or explore differention actions as well?\n",
    "- GLIE - Greedy in the limit if inifite exploration\n",
    "\n",
    "[****The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.****](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant-alpha changes to Q(St, At)\n",
    "\n",
    "- Later changes in the reward get a smaller contribution to the Action-Value estimate\n",
    "- Replacement of the 1 / N(St,At) with a constant alpha\n",
    "\n",
    "![constant-alpha pt1](images/rl14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10.]\n",
      "The running_mean function returns: 5.5\n",
      "The forgetful_mean function returns:\n",
      "1.0427 (alpha=0.01)\n",
      "1.9787 (alpha=0.02)\n",
      "2.8194 (alpha=0.03)\n",
      "3.5745 (alpha=0.04)\n",
      "4.2529 (alpha=0.05)\n",
      "4.8624 (alpha=0.06)\n",
      "5.4099 (alpha=0.07)\n",
      "5.9018 (alpha=0.08)\n",
      "6.3436 (alpha=0.09)\n",
      "6.7403 (alpha=0.1)\n",
      "7.0964 (alpha=0.11)\n",
      "7.4159 (alpha=0.12)\n",
      "7.7025 (alpha=0.13)\n",
      "7.9593 (alpha=0.14)\n",
      "8.1894 (alpha=0.15)\n",
      "8.3953 (alpha=0.16)\n",
      "8.5795 (alpha=0.17)\n",
      "8.7441 (alpha=0.18)\n",
      "8.891 (alpha=0.19)\n",
      "9.0221 (alpha=0.2)\n",
      "9.1389 (alpha=0.21)\n",
      "9.2428 (alpha=0.22)\n",
      "9.3352 (alpha=0.23)\n",
      "9.4173 (alpha=0.24)\n",
      "9.49 (alpha=0.25)\n",
      "9.5544 (alpha=0.26)\n",
      "9.6114 (alpha=0.27)\n",
      "9.6616 (alpha=0.28)\n",
      "9.706 (alpha=0.29)\n",
      "9.745 (alpha=0.3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is the sequence (corresponding to successively sampled returns). \n",
    "# Feel free to change it!\n",
    "x = np.hstack((np.ones(10), 10*np.ones(10)))\n",
    "print(\"sequence: \", x)\n",
    "\n",
    "# These are the different step sizes alpha that we will test.  \n",
    "# Feel free to change it!\n",
    "alpha_values = np.arange(0,.3,.01)+.01\n",
    "\n",
    "#########################################################\n",
    "# Please do not change any of the code below this line. #\n",
    "#########################################################\n",
    "\n",
    "def running_mean(x):\n",
    "    mu = 0\n",
    "    mean_values = []\n",
    "    for k in np.arange(0, len(x)):\n",
    "        mu = mu + (1.0/(k+1))*(x[k] - mu)\n",
    "        mean_values.append(mu)\n",
    "    return mean_values\n",
    "    \n",
    "def forgetful_mean(x, alpha):\n",
    "    mu = 0\n",
    "    mean_values = []\n",
    "    for k in np.arange(0, len(x)):\n",
    "        mu = mu + alpha*(x[k] - mu)\n",
    "        mean_values.append(mu)\n",
    "    return mean_values\n",
    "\n",
    "def print_results():\n",
    "    \"\"\"\n",
    "    prints the mean of the sequence \"x\" (as calculated by the\n",
    "    running_mean function), along with analogous results for each value of alpha \n",
    "    in \"alpha_values\" (as calculated by the forgetful_mean function).\n",
    "    \"\"\"\n",
    "    print('The running_mean function returns:', running_mean(x)[-1])  # takes the last element\n",
    "    print('The forgetful_mean function returns:')\n",
    "    for alpha in alpha_values:\n",
    "        print(np.round(forgetful_mean(x, alpha)[-1],4), \\\n",
    "        '(alpha={})'.format(np.round(alpha,2)))\n",
    "\n",
    "        \n",
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference Methods\n",
    "\n",
    "- No feedback after an episode ends anymore but more continious feedback (for example: self-driving car)\n",
    "- Can be used for episodic or continous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Prediction: TD(0)\n",
    "\n",
    "![changing the udpate step for the state/action-value function](images/rl15.png)\n",
    "- Understanding the value of a state in terms of the value of the its successor states\n",
    "\n",
    "**New State-Value Adjustment that ignores the episodic nature of the task**\n",
    "![td target into the state-value function](images/rl16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A high $\\alpha$ discounds the newer returns more. A lower $\\alpha$ takes the current value more into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD-Control\n",
    "Three algorithms to define the control functions for TD Learning:\n",
    "\n",
    "![sarsa, sarsamax and expected sarsa](images/rl20.png)\n",
    "\n",
    "\n",
    "- Sarsa and Expected Sarsa are both on-policy TD control algorithms. In this case, the same (ϵ\\epsilonϵ-greedy) policy that is evaluated and improved is also used to select actions.\n",
    "- Sarsamax is an off-policy method, where the (greedy) policy that is evaluated and improved is different from the (ϵ\\epsilonϵ-greedy) policy that is used to select actions.\n",
    "- On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Sarsamax).\n",
    "- Expected Sarsa generally achieves better performance than Sarsa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in Continous Spaces\n",
    "\n",
    "- RL Problems are mostly framed as MDPs\n",
    "\n",
    "**Reinforcement Learning Algorithms**\n",
    "- Model-based Learning (Dynamic Programming)\n",
    " - Policy Iteration\n",
    " - Value Iteration\n",
    "- Model-Free Learning (Exploratory Steps)\n",
    " - Monte Carlo\n",
    " - TD Learning\n",
    " \n",
    "**Summary of RL**\n",
    "![Summary of RL so far](images/rl21.png)\n",
    "\n",
    "**Goal of this lesson**\n",
    "- RL in Continous Spaces\n",
    "- Deep Q-Learning\n",
    "- Policy Gradients\n",
    "- Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
