{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Task:\n",
    "\n",
    "- well defined ending point\n",
    "- sparse reward (for example: chess): Reward only at the end of all actions/states\n",
    "\n",
    "Interaction ends at some time step T.\n",
    "\n",
    "S_0 A_0 R_1 S_1 A_1, ..., R_T, S_T Game Over -> Evaluate the max score reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Task\n",
    "- Agent lives forever\n",
    "\n",
    "Example: Stock Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hypothesis\n",
    "\n",
    "- All goals can be framed as the maximization of expected cumulative reward\n",
    "\n",
    "Example of the walking robot:\n",
    "\n",
    "**Actions:** Forces applied to joints\n",
    "\n",
    "**States:** The positions and velocities of the joints, statistics about ground, and foot sensor data\n",
    "\n",
    "![explanation reward for humanoid walking agent](images/rl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Return\n",
    "\n",
    "The Agent maximizes not the individual step's return, but the expected return G_t of all future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "- Reward sooner is valued higher -> Discounting future returns.\n",
    "- $\\gamma$ >= 0 and <= 1 => $\\gamma^2$ * $R_2$, $\\gamma^3$ * $R_3$, ...\n",
    "- Important for continuing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "![overview of MDPs](images/rl2.png)\n",
    "\n",
    "#### Excercise and Mathematical notation\n",
    "\n",
    "![mathematical expression of options](images/rl3.png)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A (finite) MDP is defined by:\n",
    "\n",
    "- a (finite) set of states\n",
    "- a (finite) set of actions\n",
    "- a (finite) set of rewards\n",
    "- the one-step dynamics of the environment (formula)\n",
    "- the discount rate $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy desribes the Mapping of States to Actions:\n",
    "\n",
    "- deterministic policy: $\\pi$:S -> A\n",
    "- stochastic policy: $\\pi$:S x A -> [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value function for a policy](images/rl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bellmann Equation Pt1](images/rl5.png)\n",
    "![Bellmann Equation Pt1](images/rl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-Value Function vs. Action-Value Function\n",
    "\n",
    "![State-Value Function vs. Action-Value Function comparison](images/rl7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Policy Evaluation\n",
    "\n",
    "1. Set the $v_\\pi$(s) of each state to 0\n",
    "2. Iterate through the states and Update the guesses of each state:\n",
    "V($s_1$) <- $\\frac{1}{2}$ x (-1 + V($s_2$)) + $\\frac{1}{2}$ x (-1 + V($s_3$)) | Bellmann Equation\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "![Pseudocode to update the value function with the bellmann equation](images/rl8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the action-value function:\n",
    "\n",
    "![example to calculate the action-value function](images/rl9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "The first step is the action-value function that we calculated\n",
    "\n",
    "![How to get to the policy improvement](images/rl10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
